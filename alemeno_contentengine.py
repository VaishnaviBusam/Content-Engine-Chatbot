# -*- coding: utf-8 -*-
"""Alemeno.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15l2Do75qs0N-V2tBTaha-6SyAC4v5pp1
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit langchain llama-index chromadb sentence-transformers pymupdf

folder_path = '/content/drive/My Drive/Alemeno/'

!pip install PyMuPDF langchain chromadb sentence-transformers

import fitz

def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

google_pdf = folder_path + "google.pdf"
tesla_pdf = folder_path + "tesla.pdf"
uber_pdf = folder_path + "uber.pdf"

google_text = extract_text_from_pdf(google_pdf)
tesla_text = extract_text_from_pdf(tesla_pdf)
uber_text = extract_text_from_pdf(uber_pdf)

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

google_embedding = embedding_model.encode(google_text)
tesla_embedding = embedding_model.encode(tesla_text)
uber_embedding = embedding_model.encode(uber_text)

import chromadb
from chromadb.utils import embedding_functions

client = chromadb.Client()
collection = client.get_or_create_collection("company_reports")

collection.add(
    documents=[google_text, tesla_text, uber_text],
    metadatas=[{"company": "Google"}, {"company": "Tesla"}, {"company": "Uber"}],
    ids=["google_doc", "tesla_doc", "uber_doc"]
)

def query_documents(query):
    results = collection.query(query_texts=[query], n_results=3)
    return results

query = "What are the risk factors associated with Google and Tesla?"
results = query_documents(query)
for result in results['documents']:
    print("Document:", result)

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilgpt2")
nlp_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

insights = nlp_pipeline(query, max_length=150)
print(insights[0]['generated_text'])

# Cell 1: Install packages
!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# st.title("Content Engine Chatbot")
# query = st.text_input("Ask a question:")
# if query:
#     response = f"Response for: {query}"  # Replace with actual backend call
#     st.write(response)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
# 
# # Load the language model
# tokenizer = AutoTokenizer.from_pretrained("gpt2")
# model = AutoModelForCausalLM.from_pretrained("gpt2")
# nlp_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)
# 
# st.title("Content Engine Chatbot")
# query = st.text_input("Ask a question:")
# 
# if query:
#     # Generate insights using the language model
#     insights = nlp_pipeline(query, max_length=150)
#     response = insights[0]['generated_text']  # Get the generated text
#     st.write(response)

# Install necessary packages
# !pip install pyngrok streamlit

# Authenticate with ngrok
!ngrok authtoken 2oemL8R3XfC9IUDKmoHFpModakN_32TXUjFEEPynMCLk1NT9M # Replace with your actual authtoken

import subprocess
import time
from pyngrok import ngrok


# Start Streamlit server in the background using subprocess
streamlit_process = subprocess.Popen(['streamlit', 'run', 'app.py'])

# Wait for the server to start
time.sleep(10)  # Increase the wait time if necessary

try:
    # Connect to ngrok
    public_url = ngrok.connect(addr="8501")
    print(f"Access your Streamlit app at: {public_url}")

    # Keep the script running until interrupted
    while True:
        time.sleep(1)

except KeyboardInterrupt:
    print("Shutting down...")
    # Terminate the Streamlit process when the script is interrupted
    streamlit_process.terminate()
    ngrok.kill()  # Stop the ngrok tunnel